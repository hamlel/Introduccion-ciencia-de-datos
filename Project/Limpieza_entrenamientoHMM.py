# -*- coding: utf-8 -*-
"""Untitled75.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1iKPsofkFyAH51saOoswsNt-_YgdEMudT
"""

# =======================
# FILTRAR CONOSERVER TABLA
# =======================

# Paso 1: Permite cargar el archivo desde tu PC.
# Esto abrirá un diálogo para subir archivos cuando lo ejecutes.
from google.colab import files
import os
from collections import defaultdict
import pandas as pd

# Al ejecutar, te pedirá seleccionar un archivo.
uploaded = files.upload()

# Ajusta el nombre si tu archivo tiene otro distinto:
input_file = "conoserver_250206_protein.txt"

# =======================
# Paso 2: Función para filtrar y separar las secuencias por organismo
# =======================
def filtrar_conoserver(input_file):
    """
    Lee el archivo conoserver_250206_protein.txt (o uno con formato similar),
    agrupa las secuencias por organismo y las escribe en diferentes archivos FASTA.
    Además, devuelve un diccionario con el recuento de secuencias por especie.
    """

    especies_dict = defaultdict(list)

    with open(input_file, 'r') as f:
        lines = f.read().splitlines()

    current_header = None
    current_sequence = None
    current_species = None

    for line in lines:
        if line.startswith('>'):
            # Si había una entrada previa, guardarla
            if current_header and current_sequence and current_species:
                especies_dict[current_species].append((current_header, current_sequence))

            # Procesar la nueva cabecera
            campos = line[1:].split('|')
            if len(campos) > 2:
                current_species = campos[2].strip()
            else:
                current_species = "Unknown"

            current_header = line
            current_sequence = None
        else:
            # Es una línea de secuencia
            current_sequence = line.strip()

    # Guardar la última entrada si corresponde
    if current_header and current_sequence and current_species:
        especies_dict[current_species].append((current_header, current_sequence))

    # Crear archivos FASTA para cada especie
    for especie, entradas in especies_dict.items():
        archivo_salida = especie.replace(' ', '_') + ".fasta"
        with open(archivo_salida, 'w') as out_f:
            for (header, seq) in entradas:
                out_f.write(f"{header}\n{seq}\n")

    return especies_dict

# =======================
# Paso 3: Ejecutar la función y mostrar los resultados
# =======================
especies = filtrar_conoserver(input_file)

# Crear un DataFrame para mostrar la cuenta de secuencias por especie
data = [(especie, len(entradas)) for especie, entradas in especies.items()]
df = pd.DataFrame(data, columns=["Especie", "Num_Secuencias"])

# Ordenar por número de secuencias (descendente) para ver primero las que más tienen
df.sort_values("Num_Secuencias", ascending=False, inplace=True)

# Mostrar la tabla
df.reset_index(drop=True, inplace=True)
df

from matplotlib import pyplot as plt

# Asumiendo que df['Num_Secuencias'] ya existe
df['Num_Secuencias'].plot(
    kind='line',
    figsize=(8, 4),
    title='Num_Secuencias'
)

# Ocultamos bordes superiores y derechos, como en tu ejemplo
plt.gca().spines[['top', 'right']].set_visible(False)

# Añadimos los nombres a los ejes
plt.xlabel('Número de especies')
plt.ylabel('Número de Secuencias')

# Finalmente, mostramos la gráfica
plt.show()



"""### Filtro de solo las conotoxinas"""

# =========================================
# Paso 1: Instalar lxml (un parser XML más flexible)
# =========================================
!pip install lxml

# =========================================
# Paso 2: Importar librerías y subir el archivo
# =========================================
from google.colab import files
from lxml import etree
import copy

print("Por favor, sube tu archivo XML (por ejemplo, 'conoserver_250206_protein.xml')...")
uploaded = files.upload()

# Asigna el nombre del archivo subido
xml_entrada = list(uploaded.keys())[0]

# =========================================
# Paso 3: Leer el XML como texto y parsear con lxml (recover=True)
# =========================================
with open(xml_entrada, 'r', encoding='utf-8') as f:
    xml_content = f.read()

# Este parser ignora entidades HTML no definidas (ej. &alpha;, etc.)
parser = etree.XMLParser(recover=True)
root = etree.fromstring(xml_content, parser=parser)

# =========================================
# Paso 4: Función para procesar las <entry>
# =========================================
def procesar_entries(root):
    """
    Para cada <entry>:
      - Si NO hay <region type='mature peptide'>, se conserva tal cual.
      - Si SÍ la hay:
          1) Extrae <start>, <end> y <sequence> para sacar el substring.
          2) Crea <mature_peptide_sequence> con la subsecuencia.
          3) Inserta esa etiqueta justo después de la etiqueta <id> en el XML.
    """
    # Creamos un nuevo elemento raíz con el mismo tag que el original
    nuevo_root = etree.Element(root.tag)

    for entry in root.findall("entry"):
        # Copiamos la <entry> para no alterar la original
        entry_copy = copy.deepcopy(entry)

        # Verificar si existe la región "mature peptide"
        seq_regions = entry_copy.find("sequenceRegions")
        found_mature = False
        start_val = None
        end_val = None
        full_seq = entry_copy.findtext("sequence") or ""

        if seq_regions is not None:
            for region in seq_regions.findall("region"):
                tipo = region.findtext("type")
                if tipo == "mature peptide":
                    found_mature = True
                    start_val = region.findtext("start")
                    end_val = region.findtext("end")
                    break  # Tomamos la primera coincidencia

        # Caso A: NO hay péptido maduro => se conserva intacta
        if not found_mature:
            nuevo_root.append(entry_copy)
            continue

        # Caso B: SÍ hay péptido maduro => calcular subsecuencia
        fragmento = "No se pudo extraer"
        if start_val and end_val and full_seq:
            s = int(start_val)
            e = int(end_val)
            # Toma el substring [s : e+1] (asumiendo <end> inclusivo)
            fragmento = full_seq[s : e+1]

        # Crear la etiqueta <mature_peptide_sequence>
        nuevo_tag = etree.Element("mature_peptide_sequence")
        nuevo_tag.text = fragmento

        # Insertarla inmediatamente después de <id>
        # Para ello, recorremos los hijos de entry_copy:
        children = list(entry_copy)
        inserted = False
        for i, child in enumerate(children):
            if child.tag == "id":
                # Insertamos el nuevo tag en la posición i+1
                entry_copy.insert(i+1, nuevo_tag)
                inserted = True
                break

        # Si nunca encontró <id>, lo agregamos al final (raro, pero por seguridad)
        if not inserted:
            entry_copy.append(nuevo_tag)

        # Agregar esta <entry> modificada al nuevo root
        nuevo_root.append(entry_copy)

    return nuevo_root

# =========================================
# Paso 5: Generar el nuevo archivo XML
# =========================================
nuevo_root = procesar_entries(root)
xml_salida = "resultado_filtrado.xml"
tree_salida = etree.ElementTree(nuevo_root)
tree_salida.write(xml_salida, encoding="utf-8", xml_declaration=True, pretty_print=True)

print(f"Archivo XML procesado guardado en: {xml_salida}")

# =========================================
# Paso 6: Descargar el archivo resultante
# =========================================
print("Descargando el archivo resultante...")
files.download(xml_salida)

# =========================================
# Paso 1: Instalar lxml
# =========================================
!pip install lxml

# =========================================
# Paso 2: Importar librerías y subir el archivo
# =========================================
from google.colab import files
import lxml.etree as ET

print("Por favor, sube 'resultado_filtrado(2).xml'...")
uploaded = files.upload()
xml_file = list(uploaded.keys())[0]

# =========================================
# Paso 3: Parsear el XML con lxml, preferiblemente en modo binario
#         para evitar problemas de declaración de codificación
# =========================================
parser = ET.XMLParser(recover=True)
with open(xml_file, "rb") as fxml:
    xml_content = fxml.read()

root = ET.fromstring(xml_content, parser=parser)

# =========================================
# Paso 4: Procesar cada <entry> para crear un nuevo XML simplificado
# =========================================
def extraer_minimo(root):
    """
    Para cada <entry> en el root:
      1) Lee <id>
      2) Si existe <mature_peptide_sequence>, usarlo
         Si no existe, usar <sequence>
      3) Crea un <entry> con solo esos dos nodos (<id> y la secuencia)
    Devuelve un Element con las <entry> "limpias".
    """
    # Crear un nuevo elemento raíz, por ejemplo <conoserver_minimo>
    nuevo_root = ET.Element("conoserver_minimo")

    for entry in root.findall("entry"):
        # Extraer el id
        entry_id = entry.findtext("id")
        if not entry_id:
            continue  # Si no hay <id>, lo saltamos (o podrías manejarlo distinto)

        # Intentar leer <mature_peptide_sequence>
        mseq = entry.findtext("mature_peptide_sequence")
        if mseq and mseq.strip():
            # Usar <mature_peptide_sequence>
            seq_tag_name = "mature_peptide_sequence"
            seq_value = mseq.strip()
        else:
            # No hay <mature_peptide_sequence>, usamos <sequence>
            seq_tag_name = "sequence"
            seq_value = entry.findtext("sequence") or "no_sequence"

        # Construir un nuevo <entry> con <id> y <mature_peptide_sequence>/<sequence>
        new_entry = ET.SubElement(nuevo_root, "entry")
        # <id>
        ET.SubElement(new_entry, "id").text = entry_id
        # <mature_peptide_sequence> o <sequence>
        ET.SubElement(new_entry, seq_tag_name).text = seq_value

    return nuevo_root

nuevo_root = extraer_minimo(root)

# =========================================
# Paso 5: Guardar y descargar el resultado
# =========================================
nuevo_xml = "resultado_minimo.xml"
tree_out = ET.ElementTree(nuevo_root)
tree_out.write(nuevo_xml, encoding="utf-8", xml_declaration=True, pretty_print=True)

print(f"Archivo XML generado: {nuevo_xml}")
files.download(nuevo_xml)

# ======================================
# EJEMPLO DE CÓDIGO EN GOOGLE COLAB
# ======================================

# 1) SUBIR LOS ARCHIVOS
#   "resultado_minimo.xml" y "conoserver_250206_protein.txt"
#   Ejecuta esta celda, luego selecciona los dos archivos desde tu PC.

from google.colab import files

print("Sube resultado_minimo.xml y conoserver_250206_protein.txt")
uploaded = files.upload()

# 2) PROCESAMIENTO
#   - Parsear el XML, quedarnos con la secuencia (o la maduración) por cada <id>.
#   - Leer el TXT, detectar el ID y la secuencia, y reemplazar si es maduro.

import xml.etree.ElementTree as ET

def procesar_archivos(xml_file, txt_file, txt_output="conoserver_250206_protein_matched.txt"):
    # --- LEER XML ---
    tree = ET.parse(xml_file)
    root = tree.getroot()

    xml_seqs = {}
    for entry in root.findall("entry"):
        entry_id = entry.find("id").text.strip()

        mature_tag = entry.find("mature_peptide_sequence")
        seq_tag = entry.find("sequence")

        if mature_tag is not None and mature_tag.text is not None:
            # Secuencia madura
            xml_seqs[entry_id] = ("mature", mature_tag.text.strip())
        elif seq_tag is not None and seq_tag.text is not None:
            # Secuencia normal
            xml_seqs[entry_id] = ("normal", seq_tag.text.strip())
        else:
            # Sin secuencia
            xml_seqs[entry_id] = ("none", "")

    # --- LEER TXT ---
    text_entries = {}
    with open(txt_file, "r", encoding="utf-8") as f:
        lines = f.readlines()

    # Recorremos líneas y detectamos cabeceras >Pxxxxx|..., la secuencia se asume en la siguiente línea
    i = 0
    while i < len(lines):
        line = lines[i].rstrip("\n")
        if line.startswith(">"):
            header = line
            # Extraemos el ID del primer bloque tras ">"
            # Ejemplo: ">P00001|SI|Conus striatus..."
            parts = header[1:].split("|")
            if parts:
                entry_id = parts[0]
            else:
                entry_id = header[1:]  # fallback

            seq_line = ""
            if i + 1 < len(lines):
                seq_line = lines[i+1].rstrip("\n")

            text_entries[entry_id] = (header, seq_line)
            i += 2
        else:
            i += 1

    # --- GENERAR SALIDA ---
    output_lines = []
    print("==== REPORTE DE MATCH ====")

    for entry_id, (header, original_seq) in text_entries.items():
        if entry_id in xml_seqs:
            seq_type, xml_seq = xml_seqs[entry_id]
            if seq_type == "mature":
                # Reemplazamos secuencia con la madura
                output_lines.append(header + "\n")
                output_lines.append(xml_seq + "\n")
                print(f"[REEMPLAZADO con secuencia madura]  ID {entry_id}")
            elif seq_type == "normal":
                # Verificamos coincidencia
                if xml_seq == original_seq:
                    output_lines.append(header + "\n")
                    output_lines.append(original_seq + "\n")
                    print(f"[COINCIDE con XML]  ID {entry_id}")
                else:
                    output_lines.append(header + "\n")
                    output_lines.append(original_seq + "\n")
                    print(f"[MISMATCH] ID {entry_id} - TXT vs XML")
                    print(f"   TXT: {original_seq}")
                    print(f"   XML: {xml_seq}")
            else:
                # XML sin secuencia -> se deja igual
                output_lines.append(header + "\n")
                output_lines.append(original_seq + "\n")
                print(f"[SIN SECUENCIA EN XML] ID {entry_id} -> sin cambios.")
        else:
            # ID no existe en XML -> sin cambios
            output_lines.append(header + "\n")
            output_lines.append(original_seq + "\n")
            print(f"[NO ENCONTRADO EN XML]  ID {entry_id} -> sin cambios.")

    # Guardar archivo de salida
    with open(txt_output, "w", encoding="utf-8") as out_f:
        out_f.writelines(output_lines)

    print(f"\nArchivo de salida generado: {txt_output}")

# 3) LLAMADA A LA FUNCIÓN
#    Ajusta los nombres de archivo si es necesario:
xml_filename = "resultado_minimo.xml"
txt_filename = "conoserver_250206_protein.txt"
procesar_archivos(xml_filename, txt_filename)

# ======================================
# ======================================

from google.colab import files
import gzip

# 1) SUBIR ARCHIVOs
#    Sube "clusters.clstr" y "conoserver_protein(1).fa.gz"
print("Sube clusters.clstr y conoserver_protein(1).fa.gz")
uploaded = files.upload()

# 2) PROCESAR 'clusters.clstr' PARA ENCONTRAR:
#    - Número de cluster (p.ej. 0, 1, 2, ...)
#    - ID (p.ej. P05539) de la línea con asterisco
cluster_to_id = {}  # clave: número de cluster, valor: ID con asterisco

current_cluster = None  # guardará el número de cluster actual
with open("clusters.clstr", "r") as f:
    for line in f:
        line = line.strip()
        if line.startswith(">Cluster"):
            # Ejemplo: ">Cluster 0"
            # extraemos el número de cluster
            # asumiendo el formato exacto:
            # la parte después de ">Cluster " es el número
            parts = line.split()
            if len(parts) == 2 and parts[0] == ">Cluster":
                current_cluster = parts[1]
        else:
            # líneas como:
            #   "1  430aa, >P05539|conohyal-Cn1... *"
            # Buscamos si hay un '*'
            if "*" in line:
                # parsear para obtener la parte que contiene ">Pxxxxx"
                # Ejemplo de fragmento:
                #   "1  430aa, >P05539|conohyal-Cn1... *"
                # Podemos dividir por ">"
                # splitted[1] = "P05539|conohyal-Cn1... *"
                splitted = line.split(">")
                if len(splitted) >= 2:
                    right_part = splitted[1].strip()
                    # right_part = "P05539|conohyal-Cn1... *"
                    # ahora, el ID está antes del primer '|', p.ej. "P05539"
                    id_part = right_part.split("|")[0].strip()  # = "P05539"
                    if current_cluster is not None:
                        cluster_to_id[current_cluster] = id_part

# 3) LEER 'conoserver_protein(1).fa.gz' Y GUARDAR TODAS LAS SECUENCIAS EN UN DICCIONARIO
#    clave: ID (p.ej. "P05539")
#    valor: secuencia (todas las líneas concatenadas hasta el siguiente '>')
fasta_dict = {}

with gzip.open("conoserver_protein(1).fa.gz", "rt") as f:
    current_id = None
    current_seq = []

    for line in f:
        line = line.rstrip("\n")

        if line.startswith(">"):
            # si había un ID anterior, guardamos su secuencia
            if current_id is not None:
                fasta_dict[current_id] = "".join(current_seq)

            # parseamos el nuevo header para extraer el ID
            # el header se ve algo como:
            #   >P05539|conohyal-Cn1|Conus consors|Wild type|...
            header_line = line[1:]  # quitar el ">"
            parts = header_line.split("|")
            if parts:
                current_id = parts[0].strip()
                current_seq = []
            else:
                current_id = None
        else:
            # es parte de la secuencia, la acumulamos
            if current_id:
                current_seq.append(line)

    # al finalizar, guardamos la secuencia del último ID si existía
    if current_id is not None:
        fasta_dict[current_id] = "".join(current_seq)

# 4) GENERAR ARCHIVO DE SALIDA
#    Por cada par (cluster -> ID) que encontramos con asterisco,
#    creamos un nuevo header: >Cluster X | ID
#    y la secuencia en la siguiente línea (o varias líneas si prefieres formatearla).

output_file = "clusters_extracted.fa"

with open(output_file, "w") as out_f:
    for cluster_num, pid in cluster_to_id.items():
        # construimos la cabecera
        header = f">Cluster {cluster_num}|{pid}"
        out_f.write(header + "\n")

        # obtenemos la secuencia de 'fasta_dict'
        seq = fasta_dict.get(pid, "")
        out_f.write(seq + "\n")  # la secuencia en una sola línea

print(f"Archivo de salida generado: {output_file}")
print("Si deseas descargarlo, usa:")
print("from google.colab import files")
print("files.download('clusters_extracted.fa')")

from google.colab import files
import gzip

# 1) SUBIR ARCHIVOS DESDE TU PC
print("Sube clusters.clstr y conoserver_protein(1).fa.gz")
uploaded = files.upload()

# 2) PROCESAR 'clusters.clstr' PARA OBTENER REPRESENTANTES
cluster_to_id = {}  # clave: número de cluster, valor: ID con asterisco
current_cluster = None

with open("clusters.clstr", "r") as f:
    for line in f:
        line = line.strip()
        if line.startswith(">Cluster"):
            parts = line.split()
            if len(parts) == 2 and parts[0] == ">Cluster":
                current_cluster = parts[1]
        else:
            if "*" in line:
                splitted = line.split(">")
                if len(splitted) >= 2:
                    right_part = splitted[1].strip()
                    id_part = right_part.split("|")[0].strip()
                    if current_cluster is not None:
                        cluster_to_id[current_cluster] = id_part

# 3) LEER SECUENCIAS DEL ARCHIVO .fa.gz
fasta_dict = {}
with gzip.open("conoserver_protein(1).fa.gz", "rt") as f:
    current_id = None
    current_seq = []
    for line in f:
        line = line.rstrip("\n")
        if line.startswith(">"):
            if current_id is not None:
                fasta_dict[current_id] = "".join(current_seq)
            header_line = line[1:]
            parts = header_line.split("|")
            if parts:
                current_id = parts[0].strip()
                current_seq = []
            else:
                current_id = None
        else:
            if current_id:
                current_seq.append(line)
    if current_id is not None:
        fasta_dict[current_id] = "".join(current_seq)

# 4) GENERAR ARCHIVO DE SALIDA: >ID|ClusterX
output_file = "clusters_extracted.fa"
with open(output_file, "w") as out_f:
    for cluster_num, pid in cluster_to_id.items():
        header = f">{pid}|Cluster{cluster_num}"  # <- ID primero
        out_f.write(header + "\n")
        seq = fasta_dict.get(pid, "")
        out_f.write(seq + "\n")

print(f"✅ Archivo de salida generado: {output_file}")
print("Para descargarlo, usa:")
print("from google.colab import files")
print("files.download('clusters_extracted.fa')")

from google.colab import files

# ⬆️ 1. Subir archivo
uploaded = files.upload()

# 📥 2. Leer archivo FASTA y verificar IDs duplicados
fasta_path = "clusters_extracted.fa"
seen_ids = set()
duplicate_ids = set()

with open(fasta_path, "r") as f:
    for line in f:
        line = line.strip()
        if line.startswith(">"):
            seq_id = line[1:].strip()
            if seq_id in seen_ids:
                duplicate_ids.add(seq_id)
            else:
                seen_ids.add(seq_id)

# 🚫 3. Mostrar errores si hay IDs duplicados
if duplicate_ids:
    print("Invalid parameters: Sequence -> Duplicated ID(s) found:")
    for dup_id in duplicate_ids:
        print(f"  → Duplicated ID: {dup_id}. Two sequences cannot share the same identifier")
else:
    print("✅ No duplicated IDs found.")

# 1) SUBIR ARCHIVO clusters_extracted.fa
from google.colab import files

print("Sube el archivo clusters_extracted.fa")
uploaded = files.upload()

# 2) INSTALAR CLUSTALW
!apt-get update
!apt-get install -y clustalw

# 3) CORRER CLUSTALW
!clustalw -INFILE=clusters_extracted.fa -OUTFILE=clusters_extracted.aln

# 4) DESCARGAR EL ARCHIVO DE ALINEAMIENTO
from google.colab import files
files.download("clusters_extracted.aln")

!pip install biopython

import numpy as np
from Bio import AlignIO

def forward(seq, A, B, pi):
    """
    Algoritmo forward con escalado para evitar underflow.

    Parámetros:
      seq: lista de observaciones (enteros)
      A: matriz de transición (n_states x n_states)
      B: matriz de emisión (n_states x n_symbols)
      pi: vector de probabilidades iniciales (n_states)

    Retorna:
      alpha: matriz de forward (T x n_states)
      scale: vector de factores de escalado (longitud T)
    """
    T = len(seq)
    n_states = len(pi)
    alpha = np.zeros((T, n_states))
    scale = np.zeros(T)

    # Inicialización
    alpha[0] = pi * B[:, seq[0]]
    scale[0] = np.sum(alpha[0])
    alpha[0] /= scale[0]

    # Recursión
    for t in range(1, T):
        for j in range(n_states):
            alpha[t, j] = np.dot(alpha[t-1], A[:, j]) * B[j, seq[t]]
        scale[t] = np.sum(alpha[t])
        alpha[t] /= scale[t]

    return alpha, scale

def backward(seq, A, B, scale):
    """
    Algoritmo backward utilizando los factores de escalado calculados en forward.

    Parámetros:
      seq: lista de observaciones (enteros)
      A: matriz de transición (n_states x n_states)
      B: matriz de emisión (n_states x n_symbols)
      scale: vector de escalado obtenido en forward

    Retorna:
      beta: matriz backward (T x n_states)
    """
    T = len(seq)
    n_states = A.shape[0]
    beta = np.zeros((T, n_states))

    # Inicialización
    beta[T-1] = 1.0 / scale[T-1]

    # Recursión hacia atrás
    for t in range(T-2, -1, -1):
        for i in range(n_states):
            beta[t, i] = np.sum(A[i, :] * B[:, seq[t+1]] * beta[t+1])
        beta[t] /= scale[t]

    return beta

def compute_xi(seq, A, B, alpha, beta):
    """
    Computa xi, la probabilidad de transitar del estado i al j en el tiempo t dado O y el modelo.

    xi[t, i, j] = P(q_t = i, q_(t+1) = j | O, modelo)
    """
    T = len(seq)
    n_states = A.shape[0]
    xi = np.zeros((T-1, n_states, n_states))

    for t in range(T-1):
        denom = 0.0
        for i in range(n_states):
            for j in range(n_states):
                denom += alpha[t, i] * A[i, j] * B[j, seq[t+1]] * beta[t+1, j]
        for i in range(n_states):
            for j in range(n_states):
                numer = alpha[t, i] * A[i, j] * B[j, seq[t+1]] * beta[t+1, j]
                xi[t, i, j] = numer / denom
    return xi

def baum_welch(sequences, n_states, n_symbols, n_iter=10):
    """
    Implementa el algoritmo Baum-Welch para estimar los parámetros de un HMM.

    Parámetros:
      sequences: lista de secuencias (cada secuencia es una lista de enteros)
      n_states: número de estados ocultos
      n_symbols: número de símbolos de emisión (tamaño del alfabeto)
      n_iter: número de iteraciones del algoritmo EM

    Retorna:
      pi: vector de probabilidades iniciales estimado
      A: matriz de transición estimada
      B: matriz de emisión estimada
    """
    np.random.seed(42)
    # Inicializar parámetros aleatoriamente
    pi = np.full(n_states, 1.0 / n_states)
    A = np.random.rand(n_states, n_states)
    A = A / A.sum(axis=1, keepdims=True)
    B = np.random.rand(n_states, n_symbols)
    B = B / B.sum(axis=1, keepdims=True)

    for iteration in range(n_iter):
        # Variables acumuladoras para las nuevas estimaciones
        pi_accum = np.zeros(n_states)
        A_num = np.zeros((n_states, n_states))
        A_den = np.zeros(n_states)
        B_num = np.zeros((n_states, n_symbols))
        B_den = np.zeros(n_states)

        for seq in sequences:
            T = len(seq)
            alpha, scale = forward(seq, A, B, pi)
            beta = backward(seq, A, B, scale)
            gamma = alpha * beta  # gamma[t, i] = P(q_t = i | O, modelo)
            xi = compute_xi(seq, A, B, alpha, beta)

            # Acumular para pi: promedio de las probabilidades del primer estado
            pi_accum += gamma[0]

            # Acumular para la matriz de transición
            for t in range(T-1):
                A_num += xi[t]
                A_den += gamma[t]

            # Acumular para la matriz de emisión
            for t in range(T):
                B_num[:, seq[t]] += gamma[t]
                B_den += gamma[t]

        # Actualización de parámetros (promediando sobre todas las secuencias)
        pi = pi_accum / len(sequences)
        A = A_num / A_den[:, None]
        B = B_num / B_den[:, None]

        # Opcional: se puede calcular la log-verosimilitud para monitorear la convergencia

    return pi, A, B

# ======================
# Ejemplo de uso:
# ======================

# 1. Leer el alineamiento (se asume formato clustal; cambia a "fasta" si es necesario)
alignment = AlignIO.read("clusters_extracted.aln", "clustal")
sequences_str = [str(record.seq) for record in alignment]

# 2. Crear un diccionario para mapear cada símbolo a un entero
unique_chars = set("".join(sequences_str))
symbols = sorted(unique_chars)
char_to_int = {char: i for i, char in enumerate(symbols)}
print("Símbolos encontrados:", symbols)
print("Mapping:", char_to_int)

# 3. Codificar las secuencias
sequences_encoded = []
for seq in sequences_str:
    encoded = [char_to_int[c] for c in seq]
    sequences_encoded.append(encoded)

# 4. Definir número de estados y número de símbolos
n_states = 3      # por ejemplo, 3 estados ocultos
n_symbols = len(symbols)

# 5. Entrenar el HMM con Baum-Welch
pi, A, B = baum_welch(sequences_encoded, n_states, n_symbols, n_iter=20)

# 6. Imprimir los parámetros estimados
print("\nParámetros del HMM entrenado:")
print("Probabilidades iniciales (pi):")
print(pi)
print("\nMatriz de transición (A):")
print(A)
print("\nMatriz de emisión (B):")
print(B)